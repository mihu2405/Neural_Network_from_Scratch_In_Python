{"cells":[{"cell_type":"markdown","metadata":{"id":"Liq73h6LtPTk"},"source":["<div class=\"alert alert-block alert-success\">\n","\n","<b>CODING BACKPROPAGATION FROM SCRATCH: ON A SINGLE NEURON</b>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuigIJdIrunp","outputId":"92ee0fab-236a-4dc0-d0b0-ee679d419674"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, Loss: 36.0\n","Iteration 2, Loss: 33.872400000000006\n","Iteration 3, Loss: 31.870541159999995\n","Iteration 4, Loss: 29.98699217744401\n","Iteration 5, Loss: 28.21476093975706\n","Iteration 6, Loss: 26.54726856821742\n","Iteration 7, Loss: 24.978324995835766\n","Iteration 8, Loss: 23.50210598858187\n","Iteration 9, Loss: 22.113131524656684\n","Iteration 10, Loss: 20.80624545154948\n","Iteration 11, Loss: 19.576596345362915\n","Iteration 12, Loss: 18.419619501351963\n","Iteration 13, Loss: 17.331019988822057\n","Iteration 14, Loss: 16.306756707482677\n","Iteration 15, Loss: 15.34302738607045\n","Iteration 16, Loss: 14.436254467553686\n","Iteration 17, Loss: 13.58307182852126\n","Iteration 18, Loss: 12.780312283455652\n","Iteration 19, Loss: 12.024995827503426\n","Iteration 20, Loss: 11.314318574097975\n","Iteration 21, Loss: 10.645642346368783\n","Iteration 22, Loss: 10.016484883698393\n","Iteration 23, Loss: 9.424510627071816\n","Iteration 24, Loss: 8.867522049011871\n","Iteration 25, Loss: 8.34345149591527\n","Iteration 26, Loss: 7.850353512506676\n","Iteration 27, Loss: 7.386397619917536\n","Iteration 28, Loss: 6.949861520580408\n","Iteration 29, Loss: 6.539124704714106\n","Iteration 30, Loss: 6.152662434665503\n","Iteration 31, Loss: 5.789040084776773\n","Iteration 32, Loss: 5.446907815766465\n","Iteration 33, Loss: 5.124995563854671\n","Iteration 34, Loss: 4.822108326030855\n","Iteration 35, Loss: 4.537121723962434\n","Iteration 36, Loss: 4.268977830076255\n","Iteration 37, Loss: 4.016681240318748\n","Iteration 38, Loss: 3.7792953790159114\n","Iteration 39, Loss: 3.5559390221160716\n","Iteration 40, Loss: 3.345783025909011\n","Iteration 41, Loss: 3.1480472490777887\n","Iteration 42, Loss: 2.96199765665729\n","Iteration 43, Loss: 2.786943595148845\n","Iteration 44, Loss: 2.622235228675549\n","Iteration 45, Loss: 2.467261126660823\n","Iteration 46, Loss: 2.3214459940751673\n","Iteration 47, Loss: 2.1842485358253256\n","Iteration 48, Loss: 2.0551594473580463\n","Iteration 49, Loss: 1.933699524019185\n","Iteration 50, Loss: 1.8194178821496512\n","Iteration 51, Loss: 1.7118902853146067\n","Iteration 52, Loss: 1.6107175694525138\n","Iteration 53, Loss: 1.5155241610978696\n","Iteration 54, Loss: 1.4259566831769857\n","Iteration 55, Loss: 1.3416826432012248\n","Iteration 56, Loss: 1.2623891989880334\n","Iteration 57, Loss: 1.1877819973278405\n","Iteration 58, Loss: 1.1175840812857638\n","Iteration 59, Loss: 1.0515348620817766\n","Iteration 60, Loss: 0.9893891517327431\n","Iteration 61, Loss: 0.9309162528653384\n","Iteration 62, Loss: 0.875899102320997\n","Iteration 63, Loss: 0.824133465373826\n","Iteration 64, Loss: 0.7754271775702325\n","Iteration 65, Loss: 0.7295994313758314\n","Iteration 66, Loss: 0.686480104981519\n","Iteration 67, Loss: 0.6459091307771117\n","Iteration 68, Loss: 0.6077359011481843\n","Iteration 69, Loss: 0.571818709390327\n","Iteration 70, Loss: 0.5380242236653578\n","Iteration 71, Loss: 0.5062269920467352\n","Iteration 72, Loss: 0.4763089768167732\n","Iteration 73, Loss: 0.4481591162869011\n","Iteration 74, Loss: 0.42167291251434524\n","Iteration 75, Loss: 0.3967520433847474\n","Iteration 76, Loss: 0.3733039976207086\n","Iteration 77, Loss: 0.35124173136132447\n","Iteration 78, Loss: 0.3304833450378703\n","Iteration 79, Loss: 0.3109517793461322\n","Iteration 80, Loss: 0.29257452918677546\n","Iteration 81, Loss: 0.2752833745118369\n","Iteration 82, Loss: 0.25901412707818716\n","Iteration 83, Loss: 0.24370639216786666\n","Iteration 84, Loss: 0.22930334439074565\n","Iteration 85, Loss: 0.21575151673725307\n","Iteration 86, Loss: 0.20300060209808138\n","Iteration 87, Loss: 0.1910032665140846\n","Iteration 88, Loss: 0.17971497346310206\n","Iteration 89, Loss: 0.16909381853143327\n","Iteration 90, Loss: 0.15910037385622508\n","Iteration 91, Loss: 0.14969754176132244\n","Iteration 92, Loss: 0.14085041704322837\n","Iteration 93, Loss: 0.1325261573959735\n","Iteration 94, Loss: 0.12469386149387127\n","Iteration 95, Loss: 0.11732445427958349\n","Iteration 96, Loss: 0.1103905790316602\n","Iteration 97, Loss: 0.10386649581088914\n","Iteration 98, Loss: 0.09772798590846545\n","Iteration 99, Loss: 0.09195226194127527\n","Iteration 100, Loss: 0.08651788326054576\n","Iteration 101, Loss: 0.08140467635984766\n","Iteration 102, Loss: 0.07659365998698062\n","Iteration 103, Loss: 0.07206697468175009\n","Iteration 104, Loss: 0.0678078164780584\n","Iteration 105, Loss: 0.06380037452420513\n","Iteration 106, Loss: 0.06002977238982451\n","Iteration 107, Loss: 0.05648201284158571\n","Iteration 108, Loss: 0.05314392588264784\n","Iteration 109, Loss: 0.050003119862983315\n","Iteration 110, Loss: 0.04704793547908113\n","Iteration 111, Loss: 0.044267402492267266\n","Iteration 112, Loss: 0.04165119900497413\n","Iteration 113, Loss: 0.03918961314378026\n","Iteration 114, Loss: 0.036873507006982977\n","Iteration 115, Loss: 0.034694282742870286\n","Iteration 116, Loss: 0.03264385063276675\n","Iteration 117, Loss: 0.030714599060370322\n","Iteration 118, Loss: 0.02889936625590253\n","Iteration 119, Loss: 0.027191413710178584\n","Iteration 120, Loss: 0.025584401159906914\n","Iteration 121, Loss: 0.024072363051356495\n","Iteration 122, Loss: 0.022649686395021344\n","Iteration 123, Loss: 0.021311089929075593\n","Iteration 124, Loss: 0.020051604514267282\n","Iteration 125, Loss: 0.018866554687474106\n","Iteration 126, Loss: 0.01775154130544445\n","Iteration 127, Loss: 0.016702425214292563\n","Iteration 128, Loss: 0.015715311884128023\n","Iteration 129, Loss: 0.014786536951776058\n","Iteration 130, Loss: 0.013912652617925968\n","Iteration 131, Loss: 0.013090414848206593\n","Iteration 132, Loss: 0.012316771330677665\n","Iteration 133, Loss: 0.011588850145034562\n","Iteration 134, Loss: 0.01090394910146309\n","Iteration 135, Loss: 0.010259525709566512\n","Iteration 136, Loss: 0.009653187740131247\n","Iteration 137, Loss: 0.009082684344689455\n","Iteration 138, Loss: 0.008545897699918197\n","Iteration 139, Loss: 0.008040835145853117\n","Iteration 140, Loss: 0.007565621788733238\n","Iteration 141, Loss: 0.007118493541019113\n","Iteration 142, Loss: 0.0066977905727448606\n","Iteration 143, Loss: 0.0063019511498957235\n","Iteration 144, Loss: 0.005929505836936846\n","Iteration 145, Loss: 0.005579072041973895\n","Iteration 146, Loss: 0.005249348884293189\n","Iteration 147, Loss: 0.0049391123652314335\n","Iteration 148, Loss: 0.004647210824446277\n","Iteration 149, Loss: 0.004372560664721501\n","Iteration 150, Loss: 0.004114142329436509\n","Iteration 151, Loss: 0.003870996517766834\n","Iteration 152, Loss: 0.003642220623566796\n","Iteration 153, Loss: 0.003426965384714004\n","Iteration 154, Loss: 0.003224431730477438\n","Iteration 155, Loss: 0.0030338678152061825\n","Iteration 156, Loss: 0.0028545662273275116\n","Iteration 157, Loss: 0.002685861363292431\n","Iteration 158, Loss: 0.0025271269567218426\n","Iteration 159, Loss: 0.0023777737535795973\n","Iteration 160, Loss: 0.00223724732474303\n","Iteration 161, Loss: 0.002105026007850744\n","Iteration 162, Loss: 0.0019806189707867374\n","Iteration 163, Loss: 0.001863564389613244\n","Iteration 164, Loss: 0.0017534277341871133\n","Iteration 165, Loss: 0.00164980015509665\n","Iteration 166, Loss: 0.001552296965930449\n","Iteration 167, Loss: 0.001460556215243966\n","Iteration 168, Loss: 0.001374237342923022\n","Iteration 169, Loss: 0.0012930199159562786\n","Iteration 170, Loss: 0.0012166024389232565\n","Iteration 171, Loss: 0.0011447012347829027\n","Iteration 172, Loss: 0.0010770493918072417\n","Iteration 173, Loss: 0.0010133957727514104\n","Iteration 174, Loss: 0.0009535040825818009\n","Iteration 175, Loss: 0.0008971519913012032\n","Iteration 176, Loss: 0.0008441303086152972\n","Iteration 177, Loss: 0.0007942422073761444\n","Iteration 178, Loss: 0.0007473024929201971\n","Iteration 179, Loss: 0.0007031369155886218\n","Iteration 180, Loss: 0.0006615815238773285\n","Iteration 181, Loss: 0.0006224820558161892\n","Iteration 182, Loss: 0.0005856933663174669\n","Iteration 183, Loss: 0.0005510788883680963\n","Iteration 184, Loss: 0.0005185101260655501\n","Iteration 185, Loss: 0.00048786617761505856\n","Iteration 186, Loss: 0.0004590332865180108\n","Iteration 187, Loss: 0.0004319044192847727\n","Iteration 188, Loss: 0.0004063788681050637\n","Iteration 189, Loss: 0.00038236187700005044\n","Iteration 190, Loss: 0.0003597642900693506\n","Iteration 191, Loss: 0.0003385022205262531\n","Iteration 192, Loss: 0.0003184967392931553\n","Iteration 193, Loss: 0.00029967358200094264\n","Iteration 194, Loss: 0.00028196287330469475\n","Iteration 195, Loss: 0.0002652988674923732\n","Iteration 196, Loss: 0.0002496197044235683\n","Iteration 197, Loss: 0.0002348671798921253\n","Iteration 198, Loss: 0.00022098652956051694\n","Iteration 199, Loss: 0.0002079262256634926\n","Iteration 200, Loss: 0.00019563778572677975\n","Final weights: [-3.3990955  -0.20180899  0.80271349]\n","Final bias: 0.6009044964039992\n"]}],"source":["import numpy as np\n","\n","# Initial parameters\n","weights = np.array([-3.0, -1.0, 2.0])\n","bias = 1.0\n","inputs = np.array([1.0, -2.0, 3.0])\n","target_output = 0.0\n","learning_rate = 0.001\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    return np.where(x > 0, 1.0, 0.0)\n","\n","for iteration in range(200):\n","    # Forward pass\n","    linear_output = np.dot(weights, inputs) + bias\n","    output = relu(linear_output)\n","    loss = (output - target_output) ** 2\n","\n","    # Backward pass\n","    dloss_doutput = 2 * (output - target_output)\n","    doutput_dlinear = relu_derivative(linear_output)\n","    dlinear_dweights = inputs\n","    dlinear_dbias = 1.0\n","\n","    dloss_dlinear = dloss_doutput * doutput_dlinear\n","    dloss_dweights = dloss_dlinear * dlinear_dweights\n","    dloss_dbias = dloss_dlinear * dlinear_dbias\n","\n","    # Update weights and bias\n","    weights -= learning_rate * dloss_dweights\n","    bias -= learning_rate * dloss_dbias\n","\n","    # Print the loss for this iteration\n","    print(f\"Iteration {iteration + 1}, Loss: {loss}\")\n","\n","print(\"Final weights:\", weights)\n","print(\"Final bias:\", bias)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
